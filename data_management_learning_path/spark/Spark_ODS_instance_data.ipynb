{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf4ed04",
   "metadata": {},
   "source": [
    "# Work with Instance Data\n",
    "\n",
    "In this example Notebook, we show you how to use the *Peak ODS Adapter for Apache Spark* to interact with ODS data using Spark SQL and DataFrames.\n",
    "\n",
    "The first section is on configuring the Spark framework and the *Peak ODS Adapter for Apache Spark*. The fun starts with \"Work with Instance Data\".\n",
    "\n",
    "Happy sparking!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c0c76",
   "metadata": {},
   "source": [
    "## Initialize Spark\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca34ba",
   "metadata": {},
   "source": [
    "### Configure Spark\n",
    "\n",
    "Initialize the Spark context and configure it for using the *Peak ODS Adapter for Apache Spark* as plugin. \n",
    "\n",
    "In this example we create and connect to a local Spark Master.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naughty-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().set(\"spark.jars\", \"/target/spark-ods.jar\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').config(conf = conf).getOrCreate() # or 'spark://spark-master:7077'\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-natural",
   "metadata": {},
   "source": [
    "### Initialize the Peak ODS Adapter for Apache Spark. \n",
    "\n",
    "To work with the *Peak ODS Adapter for Apache Spark*, you need to define the connection information `conInfo` to the *Peak ODS Server* together with the location of the bulk data files on disc.\n",
    "\n",
    "The connection information is then passed to the `connectionManager` to establish the ODS connection. This `odsConnection` has to be provided in all Spark ODS operations.\n",
    "\n",
    "> You have to add an override to the ODS MULTI_VOLUME symbol `DISC1` to access the bulk data files in the Spark environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functioning-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "conInfo = {\n",
    "    \"url\": \"http://nvhdemo:8080/api/\",\n",
    "    \"user\": \"sa\",\n",
    "    \"password\": \"sa\",\n",
    "    \"override.symbol.DISC1\": \"file:///data/NVH/\"\n",
    "}\n",
    "\n",
    "connectionManager = sc._jvm.com.peaksolution.sparkods.ods.ConnectionManager.instance\n",
    "odsConnection = connectionManager.createODSConnection(conInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e34696",
   "metadata": {},
   "source": [
    "## Work with Instance Data\n",
    "\n",
    "Using the \"odsinstances\" format, individual instances of ODS entities can be loaded by passing the entity name to the `load` function. The DataFrame then contains all application attributes as columns and all instances of the loaded application element as rows. \n",
    "\n",
    "So let's get all \"Projects\" of your data...\n",
    "\n",
    "\n",
    "> Only after invoking a terminal method like `show()` data will be loaded from the ODS server and transfered to the client.\n",
    "\n",
    "> In a Notebook `show()` is implicitely called and the first 20 rows will be displayed. You can avoid this by asigning the return value to a variable in your Notebook (see following examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ac45a8-0652-4cda-80b4-f193275dc011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Id</th><th>Name</th><th>Classification</th></tr>\n",
       "<tr><td>1</td><td>PMV 2PV</td><td>0</td></tr>\n",
       "<tr><td>2</td><td>PMV Model P</td><td>0</td></tr>\n",
       "<tr><td>3</td><td>PMV Summit</td><td>0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+-----------+--------------+\n",
       "| Id|       Name|Classification|\n",
       "+---+-----------+--------------+\n",
       "|  1|    PMV 2PV|             0|\n",
       "|  2|PMV Model P|             0|\n",
       "|  3| PMV Summit|             0|\n",
       "+---+-----------+--------------+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"odsinstances\").options(**odsConnection).load(\"Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3ff92",
   "metadata": {},
   "source": [
    "There were not such many attributes for \"Projects\", but this change, when working with more complex entities like \"MeaResult\".\n",
    "In this case, you can examine the schema of the DataFrame (Experts see that the ODS datatypes are mapped to Spark datatypes üòâ).\n",
    "\n",
    "> Note: The \"nullable\" property of the column field is always returned as \"true\". Please refer to your ASAM ODS datamodel to determine if a column (attribute) is nullable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tamil-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Description: string (nullable = true)\n",
      " |-- StorageType: integer (nullable = true)\n",
      " |-- Size: long (nullable = true)\n",
      " |-- MeasurementEnd: timestamp (nullable = true)\n",
      " |-- DateCreated: timestamp (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- analytic_path: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- mimeType: string (nullable = true)\n",
      " |    |    |-- location: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- MeasurementBegin: timestamp (nullable = true)\n",
      " |-- TestStep: long (nullable = true)\n",
      " |-- Classification: long (nullable = true)\n",
      " |-- TplMeaResult: long (nullable = true)\n",
      " |-- TestSequence: long (nullable = true)\n",
      " |-- TestEquipment: long (nullable = true)\n",
      " |-- UnitUnderTest: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mearesults = spark.read.format(\"odsinstances\").options(**odsConnection).load(\"MeaResult\")\n",
    "mearesults.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78f717",
   "metadata": {},
   "source": [
    "Let's start working on our Spark DataFrame by selecting columns, defining filter conditions and sorting by defined columns.\n",
    "\n",
    "Enjoy sparking.... üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd95b8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Id</th><th>Name</th><th>DateCreated</th></tr>\n",
       "<tr><td>3</td><td>Channel</td><td>2019-07-03 16:02:30</td></tr>\n",
       "<tr><td>164</td><td>Compressed - mics</td><td>2014-09-23 13:18:26</td></tr>\n",
       "<tr><td>168</td><td>Compressed - acc ...</td><td>2014-09-23 13:18:26</td></tr>\n",
       "<tr><td>172</td><td>Compressed - CAN</td><td>2014-09-23 13:18:26</td></tr>\n",
       "<tr><td>192</td><td>Compressed - acc ...</td><td>2014-09-23 13:18:26</td></tr>\n",
       "<tr><td>188</td><td>Compressed - mics</td><td>2014-09-23 13:18:26</td></tr>\n",
       "<tr><td>196</td><td>Compressed - CAN</td><td>2014-09-23 13:18:26</td></tr>\n",
       "<tr><td>2000</td><td>Channel</td><td>2014-03-17 15:48:22</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+--------------------+-------------------+\n",
       "|  Id|                Name|        DateCreated|\n",
       "+----+--------------------+-------------------+\n",
       "|   3|             Channel|2019-07-03 16:02:30|\n",
       "| 164|   Compressed - mics|2014-09-23 13:18:26|\n",
       "| 168|Compressed - acc ...|2014-09-23 13:18:26|\n",
       "| 172|    Compressed - CAN|2014-09-23 13:18:26|\n",
       "| 192|Compressed - acc ...|2014-09-23 13:18:26|\n",
       "| 188|   Compressed - mics|2014-09-23 13:18:26|\n",
       "| 196|    Compressed - CAN|2014-09-23 13:18:26|\n",
       "|2000|             Channel|2014-03-17 15:48:22|\n",
       "+----+--------------------+-------------------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mearesults.select(\"Id\", \"Name\", \"DateCreated\").where(\"Name like 'C%'\").orderBy(\"DateCreated\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bbae9",
   "metadata": {},
   "source": [
    "The central entity in our data repository is the measurement (for instance \"MeaResult\"). To make your life easier working with measurements we've created the virtual element \"measuredContext\".  \n",
    "You can use the \"measuredContext\" to generate a DataFrame containing the measurement entity (derivded from \"AoMeasurement\"), together with all its parent elements and the context elements (like UUT, Test Sequence, Test Equipment,...). \n",
    "\n",
    "The columns names of a \"measuredContext\" DataFrame are formatted as `<Entity name>___<Attribute name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d6711c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Project___Name</th><th>StructureLevel___Name</th><th>Test___Name</th><th>TestStep___Name</th><th>MeaResult___Name</th><th>vehicle___manufacturer</th></tr>\n",
       "<tr><td>PMV Model P</td><td>PBN Measurements</td><td>PBN_UNECE_R51_201...</td><td>PBN_UNECE_R51_Rig...</td><td>Channel</td><td>Peak Motor Vehicles</td></tr>\n",
       "<tr><td>PMV Summit</td><td>PBN Measurements</td><td>PBN_UNECE_R51_201...</td><td>PBN_UNECE_R51_Rig...</td><td>Channel</td><td>Peak Motor Vehicles</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------+---------------------+--------------------+--------------------+----------------+----------------------+\n",
       "|Project___Name|StructureLevel___Name|         Test___Name|     TestStep___Name|MeaResult___Name|vehicle___manufacturer|\n",
       "+--------------+---------------------+--------------------+--------------------+----------------+----------------------+\n",
       "|   PMV Model P|     PBN Measurements|PBN_UNECE_R51_201...|PBN_UNECE_R51_Rig...|         Channel|   Peak Motor Vehicles|\n",
       "|    PMV Summit|     PBN Measurements|PBN_UNECE_R51_201...|PBN_UNECE_R51_Rig...|         Channel|   Peak Motor Vehicles|\n",
       "+--------------+---------------------+--------------------+--------------------+----------------+----------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"odsinstances\").options(**odsConnection).load(\"measuredContext\") \\\n",
    ".select(\"Project___Name\", \"StructureLevel___Name\", \"Test___Name\", \"TestStep___Name\", \"MeaResult___Name\", \"vehicle___manufacturer\") \\\n",
    ".limit(100) \\\n",
    ".where(\"TestStep___Name like 'PBN%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e290cd",
   "metadata": {},
   "source": [
    "## Close the SparkContext\n",
    "It is a good practice to close the SparkContext when you‚Äôre done with it.\n",
    "\n",
    "This will ensure that all Spark-related operations are properly terminated before your program exits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa044c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dacbb1b",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright ¬© 2025 [Peak Solution GmbH](https://peak-solution.de)\n",
    "\n",
    "The training material in this repository is licensed under a Creative Commons BY-NC-SA 4.0 license. See [LICENSE](../LICENSE) file for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beaa0f6",
   "metadata": {},
   "source": [
    "**Notebook**: [üìì Back to Apache Spark Overview](overview.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
