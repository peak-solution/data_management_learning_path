{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf4ed04",
   "metadata": {},
   "source": [
    "# ASAM ODS Big Data Connector\n",
    "\n",
    "In this example Notebook, we show you how to use the *Peak ODS Adapter for Apache Spark* to export your data to the big data world (Avro/Parquet) by using the ASAM ODS Big Data Connector functionalities. \n",
    "\n",
    "The first section is on configuring the Spark framework and the *Peak ODS Adapter for Apache Spark*. The fun starts with \"Export ODS Big Data\".\n",
    "\n",
    "Happy sparking!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c0c76",
   "metadata": {},
   "source": [
    "## Initialize Spark\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca34ba",
   "metadata": {},
   "source": [
    "### Configure Spark\n",
    "\n",
    "Initialize the Spark context and configure it for using the *Peak ODS Adapter for Apache Spark* as plugin.\n",
    "\n",
    "In this example we create and connect to a local Spark Master.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naughty-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().set(\"spark.jars\", \"/target/spark-ods.jar\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "conf.set(\"spark.plugins\", \"com.peaksolution.sparkods.ods.SparkOdsPlugin\")\n",
    "conf.set(\"spark.odsplugin.host\", \"fls-test\")\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').config(conf = conf).getOrCreate() # or 'spark://spark-master:7077'\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-natural",
   "metadata": {},
   "source": [
    "### Initialize Peak ODS Adapter for Apache Spark. \n",
    "\n",
    "To work with the *Peak ODS Adapter for Apache Spark*, you need to define the connection information `conInfo` to the *Peak ODS Server* together with the location of the bulk data files on disc.\n",
    "\n",
    "The connection information is then passed to the `connectionManager` to establish the ODS connection. This `odsConnection` has to be provided in all Spark ODS operations.\n",
    "\n",
    "> You have to add an override to the ODS MULTI_VOLUME symbol `DISC1` to access the bulk data files in the Spark environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "functioning-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "conInfo = {\n",
    "    \"url\": \"http://nvhdemo:8080/api/\",\n",
    "    \"user\": \"sa\",\n",
    "    \"password\": \"sa\",\n",
    "    \"override.symbol.DISC1\": \"file:///data/NVH/\"\n",
    "}\n",
    "\n",
    "connectionManager = sc._jvm.com.peaksolution.sparkods.ods.ConnectionManager.instance\n",
    "odsConnection = connectionManager.createODSConnection(conInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8ac0e-c668-4660-8bdc-1aae4df5204c",
   "metadata": {},
   "source": [
    "## Export ODS Big Data\n",
    "\n",
    "The *Peak ODS Adapter for Apache Spark* allows exporting ASAM ODS instance and measurement (mass) data in a format suitable for big data analysis tools in the ways defined by the ASAM ODS standard: AVRO/JSON for instance data and Parquet for mass data.\n",
    "\n",
    "However, the Peak ODS Adapter for Apache Spark allows using all build-in Spark Data Sources for writing, so you're free to choose the most suitable format for your application.\n",
    "\n",
    "> Don't get confused with Data Source formats. The *Peak ODS Adapter for Apache Spark* defines the ODS connectivity as a Data Source format like all other Spark Adaptors. So you use the *Peak ODS Adapter for Apache Spark* Data Source formats to get the data into the Spark DataFrame (`read`) and the other Spark Data Source formats for exporting the DataFrame (`write`). \n",
    "\n",
    "Exporting data according to the ASAM ODS Big Data standards is a two step approach:\n",
    "\n",
    "* First you \"select\" the data to be loaded from the *Peak ODS Server*.\n",
    "* Then you specify how your Spark DataFrame is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e34696",
   "metadata": {},
   "source": [
    "### Read Instance Data using an Export Definition File\n",
    "\n",
    "To read instance data according to the ASAM ODS Big Data Connector standard, define \"mappedodsinstances\" as Data Source format.\n",
    "\n",
    "Define the export definition file by specifying the file name as \"mappingrulefile\" option.\n",
    "\n",
    "Define a specific rule of the Export Definition File to be executed - in our example \"MeaResultExport\".\n",
    "\n",
    "\n",
    "> Within an ASAM ODS Export Definition File you can select which attributes of a certain entity to be loaded, you can map the attribute to a different name (alias) and you can implicitly join other entities of your data model (schema).\n",
    "\n",
    "> You can also write to Avro using the \"odsinstance\" Data Source format of the Peak Spark ODS Adapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ac45a8-0652-4cda-80b4-f193275dc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mappedodsinstances\")\\\n",
    "    .options(**odsConnection)\\\n",
    "    .option(\"mappingrulefile\",\"mdmexportdefinition.xml\")\\\n",
    "    .load(\"MeaResultExport\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3ff92",
   "metadata": {},
   "source": [
    "After loadding the data you can export the data to Avro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tamil-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+------+-----------+-------------+--------+--------+-------------------+-------------------+---+-------------+--------------------+--------------------+-------------------+--------------------+\n",
      "|ao_element|ao_source|               ao_id|ao_iid|Description|  StorageType|MDMLinks|    Size|     MeasurementEnd|        DateCreated| Id|analytic_path|            MimeType|                Name|   MeasurementBegin|            TestStep|\n",
      "+----------+---------+--------------------+------+-----------+-------------+--------+--------+-------------------+-------------------+---+-------------+--------------------+--------------------+-------------------+--------------------+\n",
      "| MeaResult|  NVHDEMO| NVHDEMO_MeaResult_3|     3|           |     database|      []|       0|1970-01-01 00:00:00|2019-07-03 16:02:30|  3|           []|application/x-asa...|             Channel|1970-01-01 00:00:00|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   150|           |external_only|      []|     420|1970-01-01 00:00:00|1970-01-01 00:00:00|150|           []|application/x-asa...|            geometry|1970-01-01 00:00:00|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   151|           |external_only|      []|     192|2014-09-23 13:18:26|2014-09-23 13:18:26|151|           []|application/x-asa...|Slow quantity - a...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   152|           |external_only|      []|     896|2014-09-23 13:18:26|2014-09-23 13:18:26|152|           []|application/x-asa...| Slow quantity - CAN|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   153|           |external_only|      []|  150708|2014-09-23 13:18:26|2014-09-23 13:18:26|153|           []|application/x-asa...|APS - acc and mag...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   154|           |external_only|      []|   20328|2014-09-23 13:18:26|2014-09-23 13:18:26|154|           []|application/x-asa...|    Order APS - mics|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   155|           |external_only|      []|  601908|2014-09-23 13:18:26|2014-09-23 13:18:26|155|           []|application/x-asa...|          APS - mics|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   156|           |external_only|      []|    2188|2014-09-23 13:18:26|2014-09-23 13:18:26|156|           []|application/x-asa...|   1/3 Octave - mics|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   157|           |external_only|      []|   80328|2014-09-23 13:18:26|2014-09-23 13:18:26|157|           []|application/x-asa...|Order APS - acc a...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   158|           |external_only|      []|     128|2014-09-23 13:18:26|2014-09-23 13:18:26|158|           []|application/x-asa...|Order complex;c:2...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   159|           |external_only|      []|     128|2014-09-23 13:18:26|2014-09-23 13:18:26|159|           []|application/x-asa...|Order complex;c:4...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   160|           |external_only|      []|     128|2014-09-23 13:18:26|2014-09-23 13:18:26|160|           []|application/x-asa...|Order complex;c:6...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   161|           |external_only|      []|13107200|2014-09-23 13:18:26|2014-09-23 13:18:26|161|           []|application/x-asa...|   Throughput - mics|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   162|           |     database|      []|     800|2014-09-23 13:18:26|2014-09-23 13:18:26|162|           []|application/x-asa...|    Integrity - mics|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   163|           |     database|      []|     960|2014-09-23 13:18:26|2014-09-23 13:18:26|163|           []|application/x-asa...|        Level - mics|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   164|           |external_only|      []|    3200|2014-09-23 13:18:26|2014-09-23 13:18:26|164|           []|application/x-asa...|   Compressed - mics|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   165|           |external_only|      []| 1966080|2014-09-23 13:18:26|2014-09-23 13:18:26|165|           []|application/x-asa...|Throughput - acc ...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   166|           |     database|      []|     800|2014-09-23 13:18:26|2014-09-23 13:18:26|166|           []|application/x-asa...|Integrity - acc a...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   167|           |     database|      []|     960|2014-09-23 13:18:26|2014-09-23 13:18:26|167|           []|application/x-asa...|Level - acc and m...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "| MeaResult|  NVHDEMO|NVHDEMO_MeaResult...|   168|           |external_only|      []|    3840|2014-09-23 13:18:26|2014-09-23 13:18:26|168|           []|application/x-asa...|Compressed - acc ...|2014-09-23 13:18:26|{TestStep, NVHDEM...|\n",
      "+----------+---------+--------------------+------+-----------+-------------+--------+--------+-------------------+-------------------+---+-------------+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.write.format(\"avro\").save(\"mearesult.avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da959089",
   "metadata": {},
   "source": [
    "### Read ODS measurement (mass) data\n",
    "\n",
    "Use the \"ods\" format as Data Source, to load measurement data.\n",
    "\n",
    "Depending which measurements you load, your DataFrame will contain different measurement quantities and thus different columns.\n",
    "\n",
    "In addition, the DataFrame will contain a special column \"idref\" which contains a globally unique identifier to identify the measurement to which the values of the actual row belong - which is espcially useful in case several measurements are contained in the same DataFrame.\n",
    "\n",
    "You can additionally load the data as binary (packed) and in chunks (to avoid big blobs) as well as treating column (local column) names as \"case sensitive\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60d4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"ods\")\\\n",
    "    .options(**odsConnection)\\\n",
    "    .option(\"mode\", \"packed\")\\\n",
    "    .option(\"maxChunkCount\", 10000)\\\n",
    "    .option(\"caseSensitive\", \"true\")\\\n",
    "    .load(\"where MeaResult.Id = 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976ac0c",
   "metadata": {},
   "source": [
    "Having a look at the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e5cb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>data</th><th>length</th><th>flagsname</th><th>indepname</th><th>ordnum</th><th>idref</th><th>datatype</th><th>compression</th><th>min</th><th>max</th></tr>\n",
       "<tr><td>CHANNEL05</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL08</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL03</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL01</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL06</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL04</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL07</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL02</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL09</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>CHANNEL10</td><td>[1F 8B 08 00 00 0...</td><td>3001</td><td>null</td><td>X-Axis</td><td>1</td><td>NVHDEMO_SubMatrix_3</td><td>double</td><td>gzip</td><td>null</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+--------------------+------+---------+---------+------+-------------------+--------+-----------+----+----+\n",
       "|     name|                data|length|flagsname|indepname|ordnum|              idref|datatype|compression| min| max|\n",
       "+---------+--------------------+------+---------+---------+------+-------------------+--------+-----------+----+----+\n",
       "|CHANNEL05|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL08|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL03|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL01|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL06|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL04|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL07|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL02|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL09|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "|CHANNEL10|[1F 8B 08 00 00 0...|  3001|     null|   X-Axis|     1|NVHDEMO_SubMatrix_3|  double|       gzip|null|null|\n",
       "+---------+--------------------+------+---------+---------+------+-------------------+--------+-----------+----+----+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c991195",
   "metadata": {},
   "source": [
    "After loadding the data you can export the data to Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nonprofit-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"mearesult_id_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af19937-fc2d-4bb5-8b94-c10f7eefb3a3",
   "metadata": {},
   "source": [
    "Clean up and delete the parquet file afterwards for the next run of this notebook...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df1c638-1c52-47c2-b3f4-7a331751ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -r mearesult_id_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e290cd",
   "metadata": {},
   "source": [
    "## Close the SparkContext\n",
    "It is a good practice to close the SparkContext when youâ€™re done with it.\n",
    "\n",
    "This will ensure that all Spark-related operations are properly terminated before your program exits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa044c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bdfc1",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright Â© 2025 [Peak Solution GmbH](https://peak-solution.de)\n",
    "\n",
    "The training material in this repository is licensed under a Creative Commons BY-NC-SA 4.0 license. See [LICENSE](../LICENSE) file for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb375fc",
   "metadata": {},
   "source": [
    "**Notebook**: [ðŸ““ Back to Apache Spark Overview](overview.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
